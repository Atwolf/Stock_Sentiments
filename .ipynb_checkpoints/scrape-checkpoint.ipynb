{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     32\u001b[0m stock_symbols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNVDA\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 33\u001b[0m tweets_df \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstock_symbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tweets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(tweets_df)\n",
      "Cell \u001b[1;32mIn [12], line 19\u001b[0m, in \u001b[0;36mscrape_tweets\u001b[1;34m(stock_symbols, max_tweets)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m symbol \u001b[38;5;129;01min\u001b[39;00m stock_symbols:\n\u001b[0;32m     18\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m OR \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_all_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweet_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreated_at\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpublic_metrics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tweets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets\u001b[38;5;241m.\u001b[39mdata:\n\u001b[0;32m     21\u001b[0m         tweets_data\u001b[38;5;241m.\u001b[39mappend([\n\u001b[0;32m     22\u001b[0m             symbol,\n\u001b[0;32m     23\u001b[0m             tweet\u001b[38;5;241m.\u001b[39mcreated_at,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m             tweet\u001b[38;5;241m.\u001b[39mpublic_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretweet_count\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m         ])\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tweepy\\client.py:1163\u001b[0m, in \u001b[0;36mClient.search_all_tweets\u001b[1;34m(self, query, **params)\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"search_all_tweets( \\\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;124;03m    query, *, end_time=None, expansions=None, max_results=None, \\\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;124;03m    media_fields=None, next_token=None, place_fields=None, \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;124;03m.. _pagination: https://developer.twitter.com/en/docs/twitter-api/tweets/search/integrate/paginate\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1162\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m query\n\u001b[1;32m-> 1163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/2/tweets/search/all\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpansions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedia.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext_token\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplace.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoll.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msince_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msort_order\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweet.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muntil_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTweet\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tweepy\\client.py:129\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m(), json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ):\n\u001b[0;32m    127\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tweepy\\client.py:100\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(response)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(response)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFound(response)\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 Forbidden\nWhen authenticating requests to the Twitter API v2 endpoints, you must use keys and tokens from a Twitter developer App that is attached to a Project. You can create a project via the developer portal."
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "\n",
    "api_key = \"0esCMwl2tRCZHbQHaK9mkSGsk\"\n",
    "api_key_secret = \"TaPrkyToF49zd50GCzpXfNrxPQvUE1tyq1Xd0ERkwYfTr6cYwi\"\n",
    "access_token = \"746162773441208320-V43Mv8s1ARVPgMJ9UscAUlJYWQCY5lH\"\n",
    "access_token_secret = \"xnrwFtwNPciT0vHPG9KS7SpujXWh2brvrkBfaX7Qxi3qe\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAAHiugEAAAAAGGo7klF3LY0L%2BcCqpkJ%2BOkt7kkM%3DRN08i4EizLBl86DPcejP4TOhp5MssIvZduEWDtKjO6SjHN6RUI\"\n",
    "\n",
    "# Authenticate to Twitter\n",
    "auth = tweepy.OAuth1UserHandler(api_key, api_key_secret, access_token, access_token_secret)\n",
    "# api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "client = tweepy.Client(bearer_token=bearer_token, consumer_key=api_key, consumer_secret=api_key_secret, access_token=access_token, access_token_secret=access_token_secret)\n",
    "\n",
    "def scrape_tweets(stock_symbols, max_tweets=5):\n",
    "    tweets_data = []\n",
    "    for symbol in stock_symbols:\n",
    "        query = f\"#{symbol} OR {symbol}\"\n",
    "        tweets = client.search_all_tweets(query=query, tweet_fields=['created_at', 'public_metrics'], max_results=max_tweets)\n",
    "        for tweet in tweets.data:\n",
    "            tweets_data.append([\n",
    "                symbol,\n",
    "                tweet.created_at,\n",
    "                tweet.text,\n",
    "                tweet.public_metrics['like_count'],\n",
    "                tweet.public_metrics['retweet_count']\n",
    "            ])\n",
    "    \n",
    "    df = pd.DataFrame(tweets_data, columns=['Stock Symbol', 'Timestamp', 'Tweet', 'Likes', 'Retweets'])\n",
    "    return df\n",
    "\n",
    "stock_symbols = ['NVDA']\n",
    "tweets_df = scrape_tweets(stock_symbols, max_tweets=5)\n",
    "print(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (878313825.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [5], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    from news-fetch.google import google_search\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from news-fetch.google import google_search\n",
    "import time\n",
    "\n",
    "def scrape_news(stock_symbols):\n",
    "    news_data = []\n",
    "\n",
    "    for symbol in stock_symbols:\n",
    "        search = google_search(f\"{symbol} news\")\n",
    "        for entry in search.links:\n",
    "            link = entry.link\n",
    "            title = entry.title\n",
    "            published = entry.time  # Assuming the news-fetch module provides a time attribute\n",
    "\n",
    "            article_content = entry.get_text()  # news-fetch provides this method to get article text\n",
    "            if article_content:\n",
    "                news_data.append([symbol, title, link, published, article_content])\n",
    "            time.sleep(1)  # To avoid being blocked by the server\n",
    "\n",
    "    df = pd.DataFrame(news_data, columns=['Stock Symbol', 'Title', 'Link', 'Published', 'Article Content'])\n",
    "    return df\n",
    "\n",
    "stock_symbols = ['NVDA', 'AMZN', 'EMR', 'MSFT', 'UNH', 'DAL', 'LW', 'ARE', 'MDLZ', 'ELV']\n",
    "news_df = scrape_news(stock_symbols)\n",
    "print(news_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting news-fetch\n",
      "  Using cached news_fetch-0.2.8-py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from news-fetch) (4.11.1)\n",
      "Collecting pandas (from news-fetch)\n",
      "  Using cached pandas-2.2.2-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting selenium (from news-fetch)\n",
      "  Using cached selenium-4.22.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting news-please (from news-fetch)\n",
      "  Using cached news_please-1.6.11-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting newspaper3k (from news-fetch)\n",
      "  Using cached newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting fake-useragent (from news-fetch)\n",
      "  Using cached fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting chromedriver-binary (from news-fetch)\n",
      "  Using cached chromedriver_binary-128.0.6580.0.0-py3-none-any.whl\n",
      "Collecting unidecode (from news-fetch)\n",
      "  Using cached Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting cchardet (from news-fetch)\n",
      "  Using cached cchardet-2.1.7-cp310-cp310-win_amd64.whl\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from beautifulsoup4->news-fetch) (2.3.2.post1)\n",
      "Requirement already satisfied: Scrapy>=1.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from news-please->news-fetch) (2.9.0)\n",
      "Collecting PyMySQL>=0.7.9 (from news-please->news-fetch)\n",
      "  Using cached PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting psycopg2-binary>=2.8.4 (from news-please->news-fetch)\n",
      "  Using cached psycopg2_binary-2.9.9-cp310-cp310-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting hjson>=1.5.8 (from news-please->news-fetch)\n",
      "  Using cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting elasticsearch>=2.4 (from news-please->news-fetch)\n",
      "  Using cached elasticsearch-8.14.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting readability-lxml>=0.6.2 (from news-please->news-fetch)\n",
      "  Using cached readability_lxml-0.8.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting langdetect>=1.0.7 (from news-please->news-fetch)\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.4.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from news-please->news-fetch) (2.8.2)\n",
      "Collecting plac>=0.9.6 (from news-please->news-fetch)\n",
      "  Using cached plac-1.4.3-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting dotmap>=1.2.17 (from news-please->news-fetch)\n",
      "  Using cached dotmap-1.3.30-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting PyDispatcher>=2.0.5 (from news-please->news-fetch)\n",
      "  Using cached PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting warcio>=1.3.3 (from news-please->news-fetch)\n",
      "  Using cached warcio-1.7.4-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting ago>=0.0.9 (from news-please->news-fetch)\n",
      "  Using cached ago-0.0.95-py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from news-please->news-fetch) (1.16.0)\n",
      "Collecting lxml>=3.3.5 (from news-please->news-fetch)\n",
      "  Using cached lxml-5.2.2-cp310-cp310-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting hurry.filesize>=0.9 (from news-please->news-fetch)\n",
      "  Using cached hurry.filesize-0.9-py3-none-any.whl\n",
      "Collecting bs4 (from news-please->news-fetch)\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting faust-cchardet>=2.1.18 (from news-please->news-fetch)\n",
      "  Using cached faust_cchardet-2.1.19-cp310-cp310-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting boto3 (from news-please->news-fetch)\n",
      "  Using cached boto3-1.34.140-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting redis (from news-please->news-fetch)\n",
      "  Using cached redis-5.0.7-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting newspaper4k>=0.9.3.1 (from news-please->news-fetch)\n",
      "  Using cached newspaper4k-0.9.3.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting lxml-html-clean>=0.1.1 (from news-please->news-fetch)\n",
      "  Using cached lxml_html_clean-0.1.1-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: pywin32>=220 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from news-please->news-fetch) (305)\n",
      "Collecting Pillow>=3.3.0 (from newspaper3k->news-fetch)\n",
      "  Using cached pillow-10.4.0-cp310-cp310-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k->news-fetch) (6.0)\n",
      "Collecting cssselect>=0.9.2 (from newspaper3k->news-fetch)\n",
      "  Using cached cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting nltk>=3.2.1 (from newspaper3k->news-fetch)\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting requests>=2.10.0 (from newspaper3k->news-fetch)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting feedparser>=5.2.1 (from newspaper3k->news-fetch)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k->news-fetch) (3.4.4)\n",
      "Collecting feedfinder2>=0.0.4 (from newspaper3k->news-fetch)\n",
      "  Using cached feedfinder2-0.0.4-py3-none-any.whl\n",
      "Collecting jieba3k>=0.35.1 (from newspaper3k->news-fetch)\n",
      "  Using cached jieba3k-0.35.1-py3-none-any.whl\n",
      "Collecting tinysegmenter==0.3 (from newspaper3k->news-fetch)\n",
      "  Using cached tinysegmenter-0.3-py3-none-any.whl\n",
      "Collecting numpy>=1.22.4 (from pandas->news-fetch)\n",
      "  Using cached numpy-2.0.0-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->news-fetch)\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->news-fetch)\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting urllib3<3,>=1.26 (from urllib3[socks]<3,>=1.26->selenium->news-fetch)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting trio~=0.17 (from selenium->news-fetch)\n",
      "  Using cached trio-0.26.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium->news-fetch)\n",
      "  Using cached trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi>=2021.10.8 (from selenium->news-fetch)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting typing_extensions>=4.9.0 (from selenium->news-fetch)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting websocket-client>=1.8.0 (from selenium->news-fetch)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting elastic-transport<9,>=8.13 (from elasticsearch>=2.4->news-please->news-fetch)\n",
      "  Using cached elastic_transport-8.13.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k->news-fetch)\n",
      "  Using cached sgmllib3k-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\documents\\at_repo\\stock_sentiments\\.venv\\lib\\site-packages (from hurry.filesize>=0.9->news-please->news-fetch) (65.5.0)\n",
      "Collecting numpy>=1.22.4 (from pandas->news-fetch)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting click (from nltk>=3.2.1->newspaper3k->news-fetch)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>=3.2.1->newspaper3k->news-fetch)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.2.1->newspaper3k->news-fetch)\n",
      "  Using cached regex-2024.5.15-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk>=3.2.1->newspaper3k->news-fetch)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting chardet (from readability-lxml>=0.6.2->news-please->news-fetch)\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.10.0->newspaper3k->news-fetch)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.10.0->newspaper3k->news-fetch) (3.4)\n",
      "Requirement already satisfied: Twisted>=18.9.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from Scrapy>=1.1.0->news-please->news-fetch) (22.10.0)\n",
      "Requirement already satisfied: cryptography>=3.4.6 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from Scrapy>=1.1.0->news-please->news-fetch) (41.0.2)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from Scrapy>=1.1.0->news-please->news-fetch) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from Scrapy>=1.1.0->news-please->news-fetch) (1.8.1)\n",
      "Requirement already satisfied: pyOpenSSL>=21.0.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from Scrapy>=1.1.0->news-please->news-fetch) (23.2.0)\n",
      "Collecting queuelib>=1.4.2 (from Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from Scrapy>=1.1.0->news-please->news-fetch) (23.1.0)\n",
      "Collecting w3lib>=1.17.0 (from Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting zope.interface>=5.1.0 (from Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached zope.interface-6.4.post2-cp310-cp310-win_amd64.whl.metadata (44 kB)\n",
      "Collecting protego>=0.1.15 (from Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting itemadapter>=0.1.0 (from Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached itemadapter-0.9.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from Scrapy>=1.1.0->news-please->news-fetch) (21.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from tldextract>=2.0.1->newspaper3k->news-fetch) (1.5.1)\n",
      "Collecting filelock>=3.0.8 (from tldextract>=2.0.1->newspaper3k->news-fetch)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium->news-fetch)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium->news-fetch)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium->news-fetch)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium->news-fetch) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from trio~=0.17->selenium->news-fetch) (1.15.1)\n",
      "Collecting exceptiongroup (from trio~=0.17->selenium->news-fetch)\n",
      "  Using cached exceptiongroup-1.2.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->news-fetch)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium->news-fetch)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.140 (from boto3->news-please->news-fetch)\n",
      "  Using cached botocore-1.34.140-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->news-please->news-fetch)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->news-please->news-fetch)\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting async-timeout>=4.0.3 (from redis->news-please->news-fetch)\n",
      "  Using cached async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pycparser in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from cffi>=1.14->trio~=0.17->selenium->news-fetch) (2.21)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting constantly>=15.1 (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting incremental>=21.3.0 (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached incremental-22.10.0-py2.py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting Automat>=0.8.0 (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached Automat-22.10.0-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting twisted-iocpsupport<2,>=1.0.2 (from Twisted>=18.9.0->Scrapy>=1.1.0->news-please->news-fetch)\n",
      "  Using cached twisted_iocpsupport-1.0.4-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium->news-fetch)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.2.1->newspaper3k->news-fetch) (0.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages (from packaging->Scrapy>=1.1.0->news-please->news-fetch) (3.0.9)\n",
      "Using cached fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Using cached news_please-1.6.11-py3-none-any.whl (95 kB)\n",
      "Using cached newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Using cached pandas-2.2.2-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Using cached selenium-4.22.0-py3-none-any.whl (9.4 MB)\n",
      "Using cached Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Using cached dotmap-1.3.30-py3-none-any.whl (11 kB)\n",
      "Using cached elasticsearch-8.14.0-py3-none-any.whl (480 kB)\n",
      "Using cached faust_cchardet-2.1.19-cp310-cp310-win_amd64.whl (118 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Using cached hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Using cached lxml-5.2.2-cp310-cp310-win_amd64.whl (3.8 MB)\n",
      "Using cached lxml_html_clean-0.1.1-py3-none-any.whl (11 kB)\n",
      "Using cached newspaper4k-0.9.3.1-py3-none-any.whl (296 kB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Using cached pillow-10.4.0-cp310-cp310-win_amd64.whl (2.6 MB)\n",
      "Using cached plac-1.4.3-py2.py3-none-any.whl (22 kB)\n",
      "Using cached psycopg2_binary-2.9.9-cp310-cp310-win_amd64.whl (1.2 MB)\n",
      "Using cached PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Using cached PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached readability_lxml-0.8.1-py3-none-any.whl (20 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached trio-0.26.0-py3-none-any.whl (475 kB)\n",
      "Using cached trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached warcio-1.7.4-py2.py3-none-any.whl (40 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached boto3-1.34.140-py3-none-any.whl (139 kB)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Using cached redis-5.0.7-py3-none-any.whl (252 kB)\n",
      "Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached botocore-1.34.140-py3-none-any.whl (12.4 MB)\n",
      "Using cached charset_normalizer-3.3.2-cp310-cp310-win_amd64.whl (100 kB)\n",
      "Using cached elastic_transport-8.13.1-py3-none-any.whl (64 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached itemadapter-0.9.0-py3-none-any.whl (11 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Using cached regex-2024.5.15-cp310-cp310-win_amd64.whl (268 kB)\n",
      "Using cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Using cached w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached zope.interface-6.4.post2-cp310-cp310-win_amd64.whl (206 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached exceptiongroup-1.2.1-py3-none-any.whl (16 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Using cached constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Using cached incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Using cached twisted_iocpsupport-1.0.4-cp310-cp310-win_amd64.whl (46 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Using cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: twisted-iocpsupport, tinysegmenter, sortedcontainers, sgmllib3k, pytz, PyDispatcher, plac, jieba3k, incremental, hjson, faust-cchardet, fake-useragent, dotmap, chromedriver-binary, cchardet, ago, zope.interface, websocket-client, warcio, w3lib, urllib3, unidecode, tzdata, typing_extensions, tqdm, regex, queuelib, pysocks, PyMySQL, pyasn1, psycopg2-binary, protego, Pillow, numpy, lxml, langdetect, joblib, jmespath, itemadapter, hyperlink, hurry.filesize, h11, filelock, feedparser, exceptiongroup, cssselect, constantly, click, charset-normalizer, chardet, certifi, attrs, async-timeout, wsproto, requests, redis, readability-lxml, pyasn1-modules, pandas, outcome, nltk, lxml-html-clean, elastic-transport, bs4, botocore, Automat, trio, s3transfer, feedfinder2, elasticsearch, trio-websocket, boto3, selenium, newspaper4k, newspaper3k, news-please, news-fetch\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 1.4.2\n",
      "    Not uninstalling websocket-client at c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages, outside environment c:\\Users\\Admin\\Documents\\at_repo\\Stock_Sentiments\\.venv\n",
      "    Can't uninstall 'websocket-client'. No files were found to uninstall.\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.1.0\n",
      "    Not uninstalling attrs at c:\\users\\admin\\appdata\\roaming\\python\\python311\\site-packages, outside environment c:\\Users\\Admin\\Documents\\at_repo\\Stock_Sentiments\\.venv\n",
      "    Can't uninstall 'attrs'. No files were found to uninstall.\n",
      "Successfully installed Automat-22.10.0 Pillow-10.4.0 PyDispatcher-2.0.7 PyMySQL-1.1.1 ago-0.0.95 async-timeout-4.0.3 attrs-23.2.0 boto3-1.34.140 botocore-1.34.140 bs4-0.0.2 cchardet-2.1.7 certifi-2024.7.4 chardet-5.2.0 charset-normalizer-3.3.2 chromedriver-binary-128.0.6580.0.0 click-8.1.7 constantly-23.10.4 cssselect-1.2.0 dotmap-1.3.30 elastic-transport-8.13.1 elasticsearch-8.14.0 exceptiongroup-1.2.1 fake-useragent-1.5.1 faust-cchardet-2.1.19 feedfinder2-0.0.4 feedparser-6.0.11 filelock-3.15.4 h11-0.14.0 hjson-3.1.0 hurry.filesize-0.9 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.9.0 jieba3k-0.35.1 jmespath-1.0.1 joblib-1.4.2 langdetect-1.0.9 lxml-5.2.2 lxml-html-clean-0.1.1 news-fetch-0.2.8 news-please-1.6.11 newspaper3k-0.2.8 newspaper4k-0.9.3.1 nltk-3.8.1 numpy-1.26.4 outcome-1.3.0.post0 pandas-2.2.2 plac-1.4.3 protego-0.3.1 psycopg2-binary-2.9.9 pyasn1-0.6.0 pyasn1-modules-0.4.0 pysocks-1.7.1 pytz-2024.1 queuelib-1.7.0 readability-lxml-0.8.1 redis-5.0.7 regex-2024.5.15 requests-2.32.3 s3transfer-0.10.2 selenium-4.22.0 sgmllib3k-1.0.0 sortedcontainers-2.4.0 tinysegmenter-0.3 tqdm-4.66.4 trio-0.26.0 trio-websocket-0.11.1 twisted-iocpsupport-1.0.4 typing_extensions-4.12.2 tzdata-2024.1 unidecode-1.3.8 urllib3-2.2.2 w3lib-2.2.1 warcio-1.7.4 websocket-client-1.8.0 wsproto-1.2.0 zope.interface-6.4.post2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matplotlib 3.6.3 requires fonttools>=4.22.0, which is not installed.\n",
      "matplotlib 3.6.3 requires kiwisolver>=1.0.1, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install news-fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.11' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Admin/Documents/at_repo/Stock_Sentiments/.venv/Scripts/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!c:/Users/Admin/Documents/at_repo/Stock_Sentiments/.venv/Scripts/python.exe -m pip install ipykernel -U --user --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
